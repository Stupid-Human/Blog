---
layout: post

title: NLP课程_张家俊_数学基础

date: 2020-06-29

categories: NLP

tags: [技术]

description: 
---


# 数学基础

[toc]

## 一、绪论

略

## 二、数学基础

### 二项分布

> 指在只有两个结果的n次独立的伯努利试验中，所期望的结果出现次数的概率。

### 熵

![Alt](https://user-images.githubusercontent.com/35519242/85991804-88828480-ba26-11ea-8bdd-56807fa01c5e.png)

> 其实本质就是log2 1/p(x) 的期望。

> 代表信源发出一个符号所提供的平均信息量。熵描述一个随机变量的不确定性的大小，如果熵越大，那么越不确定，则需要更多的信息量才能确定其值。

> **事实上，所有可能出现的情况都是等概率的时候，熵是最大的。**

#### 联合熵

![Alt](https://user-images.githubusercontent.com/35519242/85991880-a9e37080-ba26-11ea-883d-8b920f074ad8.png) 

#### 条件熵

![](https://user-images.githubusercontent.com/35519242/85992008-ddbe9600-ba26-11ea-9972-8847f399f9e2.png) 

#### 连锁规则

> H(X,Y) = H(X) + H(Y|X)

注意：H(X|Y) != H(Y|X)

#### 熵率 

![](https://user-images.githubusercontent.com/35519242/85992057-ee6f0c00-ba26-11ea-8aaf-a60c78aef7f5.png)

#### **相对熵**

![](https://user-images.githubusercontent.com/35519242/85992092-fcbd2800-ba26-11ea-8cec-6ba33bdfedfb.png) 

>相对熵常被用以衡量两个随机分布的差距。当两个随机分布相同时，其相对熵为0。当两个随机分布的差别增加时，其相对熵也增加。

> 由于不对称，常用两个相对熵来求和衡量。D(p||q) + D(q||p)

#### 交叉熵

![](https://user-images.githubusercontent.com/35519242/85992155-0e063480-ba27-11ea-9f64-4d699c3eae3a.png) 

>**交叉熵的概念用以衡量估计模型与真实概率分布之间的差异。**

 ![](https://user-images.githubusercontent.com/35519242/85992212-1e1e1400-ba27-11ea-8851-78759ff711d3.png)

>我们可以根据模型 q 和一个含有大量数据的 L 的样本来计算交叉熵。在设计模型 q 时，我们的
>目的是使交叉熵最小，从而使模型最接近真实的概率分布 p(x)。

#### 困惑度 

![](https://user-images.githubusercontent.com/35519242/85992254-2f672080-ba27-11ea-9e0b-ba821d1746a8.png)

>语言模型设计的任务就是寻找困惑度最小的模型，使其最接近真实的语言。**其实困惑都和交叉熵是等价的，区别就在于一个指数问题而已**。

#### 互信息

如果 (X, Y) ~ p(x, y)，X, Y 之间的互信息 I(X; Y) 定义为：I (X; Y) = H(X) – H(X | Y)

![](https://user-images.githubusercontent.com/35519242/85992296-3e4dd300-ba27-11ea-81c5-6c6c27e8b320.png) 

> 互信息I (X; Y) 是在知道了 Y 的值以后 X 的不确定性的减少量，即Y 的值透露了多少关于 X 的信息量。

![](https://user-images.githubusercontent.com/35519242/85992334-4c9bef00-ba27-11ea-9842-7a0bc206b46c.png)

例子：汉语分词

>互信息值越大，表示两个汉字之间的结合越紧密，越可能成词。反之，断开的可能性越大。
